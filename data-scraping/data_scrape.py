import pandas as pd
import time
import re
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError

# å‡è®¾ youtube æœåŠ¡å¯¹è±¡åœ¨å¤–éƒ¨å·²ç»å»ºç«‹ï¼Œæˆ–è€…å»ºè®®ä½œä¸ºå‚æ•°ä¼ å…¥
# youtube = build('youtube', 'v3', developerKey=API_KEY)

def get_channel_title(youtube, video_id: str) -> str:
    """
    æ ¹æ® video_id è·å–å‘å¸ƒè¯¥è§†é¢‘çš„åšä¸»åå­—ï¼ˆchannelTitleï¼‰
    """
    try:
        request = youtube.videos().list(
            part="snippet",
            id=video_id
        )
        response = request.execute()

        items = response.get("items", [])
        if not items:
            return "UnknownChannel"

        return items[0]["snippet"].get("channelTitle", "UnknownChannel")

    except HttpError as e:
        print(f"[Channel Title Error] video={video_id}: {e}")
        return "UnknownChannel"


def sanitize_filename(s: str) -> str:
    """
    æ¸…ç†å­—ç¬¦ä¸²ï¼Œä½¿å…¶é€‚åˆåšæ–‡ä»¶å
    """
    if not s: return "untitled"
    # æ›¿æ¢éæ³•å­—ç¬¦
    s = re.sub(r'[\\/:*?"<>|]+', "_", s)
    # æ›¿æ¢æ¢è¡Œç¬¦ç­‰
    s = s.replace('\n', ' ').replace('\r', '')
    s = s.strip()
    return s[:50] # æˆªæ–­ä¸€ä¸‹é˜²æ­¢å¤ªé•¿

def scrape_all_comments_for_single_video(
    youtube, # <--- å»ºè®®æŠŠ youtube å¯¹è±¡ä¼ è¿›æ¥
    video_id: str,
    top_max_pages: int = 50,
    reply_max_pages: int = 20,
    sleep_sec: float = 0.2 # ç¨å¾®å¢åŠ ä¸€ç‚¹å»¶æ—¶ï¼Œé˜²æ­¢è§¦å‘é™æµ
) -> pd.DataFrame:
    
    rows = []
    
    # ---------- Step 1: é¡¶å±‚è¯„è®ºå¾ªç¯ ----------
    page_token = None
    top_pages = 0

    print(f"ğŸš€ Scrapping Video: {video_id}")

    while True:
        try:
            # è¯·æ±‚ä¸»æ¥¼
            request = youtube.commentThreads().list(
                part="snippet", 
                videoId=video_id,
                maxResults=100,
                pageToken=page_token,
                textFormat="plainText"
            )
            response = request.execute()

        except HttpError as e:
            print(f"[Top Error] video={video_id}: {e}")
            break

        items = response.get("items", [])
        if not items:
            break # è¿™ä¸€é¡µæ²¡ä¸œè¥¿äº†ï¼Œç›´æ¥é€€

        # --- éå†è¿™ä¸€é¡µçš„æ¯ä¸€ä¸ªä¸»æ¥¼ ---
        for item in items:
            top_comment_id = item["snippet"]["topLevelComment"]["id"]
            top_snip = item["snippet"]["topLevelComment"]["snippet"]
            reply_count = item["snippet"].get("totalReplyCount", 0)

            # 1. ä¿å­˜é¡¶å±‚è¯„è®º
            rows.append({
                "video_id": video_id,
                "comment_id": top_comment_id,
                "parent_comment_id": None,
                "text": top_snip.get("textDisplay"),
                "author": top_snip.get("authorDisplayName"),
                "like_count": top_snip.get("likeCount", 0),
                "published_at": top_snip.get("publishedAt"),
                "comment_type": "top",
            })

            # 2. å¦‚æœæœ‰å›å¤ï¼Œå»æŠ“å›å¤ (åµŒå¥—å¾ªç¯)
            if reply_count > 0:
                reply_token = None
                reply_pages = 0
                
                # print(f"   -> å‘ç° {reply_count} æ¡å›å¤ï¼Œæ­£åœ¨å±•å¼€...") # è°ƒè¯•ç”¨

                while True:
                    try:
                        reply_request = youtube.comments().list(
                            part="snippet",
                            parentId=top_comment_id,
                            maxResults=100,
                            pageToken=reply_token,
                            textFormat="plainText"
                        )
                        reply_resp = reply_request.execute()
                    except HttpError as e:
                        print(f"[Reply Error] parent={top_comment_id}: {e}")
                        break

                    reply_items = reply_resp.get("items", [])
                    for r in reply_items:
                        r_snip = r["snippet"]
                        rows.append({
                            "video_id": video_id,
                            "comment_id": r["id"],
                            "parent_comment_id": top_comment_id,
                            "text": r_snip.get("textDisplay"),
                            "author": r_snip.get("authorDisplayName"),
                            "like_count": r_snip.get("likeCount", 0),
                            "published_at": r_snip.get("publishedAt"),
                            "comment_type": "reply",
                        })

                    # å›å¤ç¿»é¡µé€»è¾‘
                    reply_token = reply_resp.get("nextPageToken")
                    reply_pages += 1
                    
                    if not reply_token or (reply_max_pages and reply_pages >= reply_max_pages):
                        break
                    
                    time.sleep(sleep_sec) # ä¼‘æ¯ä¸€ä¸‹

        # --- âš ï¸ å…³é”®ä¿®æ­£ï¼šä¸»æ¥¼ç¿»é¡µé€»è¾‘å¿…é¡»åœ¨ for å¾ªç¯å¤–é¢ ---
        page_token = response.get("nextPageToken")
        top_pages += 1
        
        # æ‰“å°è¿›åº¦
        if top_pages % 5 == 0:
            print(f"   ...Scrapped {len(rows)} Comments (Page {top_pages})")

        if not page_token:
            break

        if top_max_pages and top_pages >= top_max_pages:
            print(f"Approached Max Pages {top_max_pages}, Scrapping Abortingã€‚")
            break
            
        time.sleep(sleep_sec)

    # è½¬æ¢ DataFrame
    df = pd.DataFrame(rows)
    if not df.empty:
        # å»é‡
        df = df.drop_duplicates(subset=["video_id", "comment_id"]).reset_index(drop=True)
    
    print(f"âœ… Scraped {len(df)} Comments.")
    return df